{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlMkm_Zc20ZT"
      },
      "source": [
        "\n",
        "# IMPORTS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vC_BhOFwWJUP",
        "outputId": "b84c5d03-98e0-41c3-db66-cf1f9f8dee81",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade fastai\n",
        "!pip install wget\n",
        "!pip install torchsummary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlGv_xARWZFw",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vrf1TwEWcbK",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tqdm\n",
        "import h5py\n",
        "import random\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torchsummary import summary\n",
        "from torchvision.utils import make_grid\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import fastai\n",
        "from fastai.vision import *\n",
        "from fastai.data.core import DataLoaders\n",
        "from fastai.basics import *\n",
        "from fastai.vision.core import *\n",
        "from fastai.vision.data import *\n",
        "from fastai.vision.augment import *\n",
        "from fastai.vision import models\n",
        "from fastai.vision.all import *\n",
        "from fastai.layers import PixelShuffle_ICNR,ConvLayer,NormType\n",
        "\n",
        "from sklearn.metrics import accuracy_score,max_error,explained_variance_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7HOn3hWU8wM"
      },
      "source": [
        "# DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnSt0OMiTL5K",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"/content/drive/My Drive/DeepDrive/dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Aiu4G24TUtH",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "  def __init__(self,df,image_size,tsfm):\n",
        "    self.dataset=df.values\n",
        "    self.image_size=image_size\n",
        "\n",
        "  def __len__(self):\n",
        "    #print(len(self.dataset))\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    img=cv2.imread(self.dataset[idx][0])\n",
        "    msk=cv2.imread(self.dataset[idx][1])[:,:,0]\n",
        "    #img=cv2.resize(img,(self.image_size,self.image_size))\n",
        "    #msk=cv2.resize(msk,(self.image_size,self.image_size))\n",
        "    img=np.transpose(img,(2,0,1))\n",
        "    #X = tsfm(img)\n",
        "    X=torch.cuda.FloatTensor(img)\n",
        "    y=torch.cuda.LongTensor(msk)\n",
        "    return X,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGbQxoizTn1L",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "tsfm = transforms.Compose([\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "                          ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTAMj2yBTs4J",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "train_df=df[0:3000]\n",
        "valid_df=df[28000:]\n",
        "train_dataset=ImageDataset(df=train_df,image_size=224,tsfm=tsfm)\n",
        "valid_dataset=ImageDataset(df=valid_df,image_size=224,tsfm=tsfm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcXIJ9mea1v7"
      },
      "source": [
        "# LOSS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bX8fVyVYa4kZ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def cse_loss(predictions,target):\n",
        "  cse_loss=nn.CrossEntropyLoss()\n",
        "  print(predictions.shape)\n",
        "  print(target.shape)\n",
        "  loss = cse_loss(predictions, target)\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTwnbgYgAKm0"
      },
      "source": [
        "# ACCURACY METRIC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5v3FLLG06LT",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "void_code = 0\n",
        "\n",
        "def acc_camvid(input, target):\n",
        "    target = target.squeeze(1)\n",
        "    mask = target != void_code\n",
        "    return (input.argmax(dim=1)[mask]==target[mask]).float().mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KZgHVPK3igL"
      },
      "source": [
        "# DATA-LOADERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOMbdzhQUmM9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "dls = DataLoaders.from_dsets(train_DepthDataset, valid_DepthDataset, bs=5, num_workers=0, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBIC_B8IVsLV"
      },
      "source": [
        "# FASTAI D-UNET\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "d3e16f52c67f47f79c0d2778b5a1082f",
            "a2d5078ac55745f39c962cc58da5c9ed",
            "e15747ea95e44b25b65495fb2b4291eb",
            "1ce2269d4dc3421795527c77add8c2e8",
            "8224f5be8e0c45b4917a1e83471cc889",
            "edecba93855141c4a73e62251a05d408",
            "b1e4d8c4f9974117b9c838ab1df75a6d",
            "b4c9f462a3fd4987a5dcd1d52f1d02f6"
          ]
        },
        "id": "LFL-i6bMWXOS",
        "outputId": "fdbc2984-8a24-4ea5-9501-09086d19923b",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "learn=unet_learner(dls, arch=resnet34,loss_func=cse_loss,metrics=acc_camvid, pretrained=True,n_in=3, n_out=8,opt_func=Adam, self_attention=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2Lya-U7FH3mr",
        "outputId": "0e2b9f0f-6352-4912-cf29-19ef102f960b",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "learn.model.load_state_dict(torch.load(\"/content/drive/My Drive/DeepDrive/89.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4jaKrGqdWkhv",
        "outputId": "9a5b4699-5fcb-49a7-a7d4-428154c6c1b2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "learn.model.to(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdRKqVrH374k"
      },
      "source": [
        "# TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hCywIYFMWo-C",
        "outputId": "4e34be05-0ad8-4308-e416-c89e71f8731d",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "learn.lr_find()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2WwHSiZiasYu",
        "outputId": "a076fe00-84fc-4f13-b871-703b6fc07209",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "learn.fit_one_cycle(n_epoch=4,lr_max=0.00010964782268274575)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHL0-KRyezSY",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "torch.save(learn.model, \"/content/drive/My Drive/DeepDrive/model.pth\")\n",
        "torch.save(learn.model.state_dict(),\"/content/drive/My Drive/DeepDrive/91.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sP-oUkkuleld",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toFaKiox5N9Z"
      },
      "source": [
        "# OUR MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxWBUOjG-dNJ"
      },
      "source": [
        "## ENCODER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1X1N7hSLYCC",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class res_block(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,strides,identity_fn=None):\n",
        "        super(res_block,self).__init__()\n",
        "        self.conv1=nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=strides,padding=1,bias=False)\n",
        "        self.bn1=nn.BatchNorm2d(out_channels)\n",
        "        self.conv2=nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=1,padding=1,bias=False)\n",
        "        self.bn2=nn.BatchNorm2d(out_channels)\n",
        "        self.relu=nn.ReLU()\n",
        "        self.downsample=identity_fn\n",
        "    def forward(self,x):\n",
        "        identity=x\n",
        "        x=self.conv1(x)\n",
        "        x=self.relu(x)\n",
        "        x=self.bn1(x)\n",
        "        x=self.conv2(x)\n",
        "        x=self.relu(x)\n",
        "        x=self.bn2(x)\n",
        "        \n",
        "        if self.downsample!=None:\n",
        "            identity=self.downsample(identity)\n",
        "        x=torch.add(x,identity)\n",
        "        x=self.relu(x)\n",
        "        \n",
        "        return x\n",
        "        \n",
        "            \n",
        "class encoder(nn.Module):\n",
        "    def __init__(self,res_block,layers,image_channels):\n",
        "        super(encoder,self).__init__()\n",
        "        self.conv=nn.Conv2d(image_channels,64, kernel_size=7, stride=2,padding=3,bias=False)\n",
        "        self.bn=nn.BatchNorm2d(64)\n",
        "        self.relu=nn.ReLU()\n",
        "        self.maxpool=nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
        "        self.layers=layers\n",
        "        self.layer1=self.res_layer(res_block,count=self.layers[0],in_channels=64,out_channels=64,stride=1)\n",
        "        self.layer2=self.res_layer(res_block,count=self.layers[1],in_channels=64,out_channels=128,stride=2)\n",
        "        self.layer3=self.res_layer(res_block,count=self.layers[2],in_channels=128,out_channels=256,stride=2)\n",
        "        self.layer4=self.res_layer(res_block,count=self.layers[3],in_channels=256,out_channels=512,stride=2)\n",
        "    def forward(self,x):\n",
        "        #print(\"input\",x.shape)\n",
        "        skip_outputs=[]\n",
        "        in_channel=3\n",
        "        x=self.conv(x)\n",
        "        x=self.bn(x)\n",
        "        x=self.relu(x)\n",
        "        #print(x.shape)\n",
        "        x_skip1=x\n",
        "        x=self.maxpool(x)\n",
        "        x=self.layer1(x)\n",
        "        x_skip2=x\n",
        "        x=self.layer2(x)\n",
        "        x_skip3=x\n",
        "        x=self.layer3(x)\n",
        "        x_skip4=x\n",
        "        x=self.layer4(x)\n",
        "        return x,x_skip1,x_skip2,x_skip3,x_skip4\n",
        "        \n",
        "    def res_layer(self,res_block,count,in_channels,out_channels,stride=1):\n",
        "        layers=[]\n",
        "        if stride!=1:\n",
        "            downsample=nn.Sequential(nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride,padding=0,bias=False),\n",
        "                                      nn.BatchNorm2d(out_channels))\n",
        "            \n",
        "        else:\n",
        "            downsample=None\n",
        "        layers.append(res_block(in_channels,out_channels,stride,downsample))\n",
        "        \n",
        "        for i in range(count-1):\n",
        "            layers.append(res_block(out_channels,out_channels,1,None))\n",
        "        return nn.Sequential(*layers)\n",
        "            \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDy_QxTx-mwp"
      },
      "source": [
        "## ATTENTION MODULE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chwWiALwLYCH",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class attention(nn.Module):\n",
        "    def __init__(self,in_channels):\n",
        "        super(attention,self).__init__()\n",
        "        self.relu=nn.ReLU()\n",
        "        self.conv=nn.Conv2d(in_channels,1,kernel_size=1,stride=1,padding=0)\n",
        "        self.sigmoid=nn.Sigmoid()\n",
        "    def forward(self,x,g):\n",
        "        x_=torch.add(x,g)\n",
        "        x_=self.relu(x)\n",
        "        x_=self.conv(x)\n",
        "        x_=self.sigmoid(x)\n",
        "        x=torch.mul(x,x_)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQeOw7gw-sqd"
      },
      "source": [
        "## UNET-BLOCK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxoMIfpWLYCM",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class unet_block(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,skip_channels):\n",
        "        super(unet_block,self).__init__()\n",
        "        final_channel=out_channels+skip_channels\n",
        "        self.Pixelshuf=PixelShuffle_ICNR(in_channels,out_channels)\n",
        "        self.conv=nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=1,padding=1)\n",
        "        self.attention=attention(out_channels)\n",
        "        self.up_conv1=nn.Conv2d(final_channel,final_channel,kernel_size=3,stride=1,padding=1)\n",
        "        self.up_conv2=nn.Conv2d(final_channel,final_channel,kernel_size=3,stride=1,padding=1)\n",
        "        self.relu=nn.ReLU()\n",
        "        \n",
        "    def forward(self,x,skip):\n",
        "        x=self.Pixelshuf(x)\n",
        "        x=self.conv(x)\n",
        "        skip=self.attention(skip,x)\n",
        "        x=torch.cat([x, skip],dim=1)\n",
        "        x=self.up_conv1(x)\n",
        "        x=self.relu(x)\n",
        "        x=self.up_conv2(x)\n",
        "        x=self.relu(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3V9j15GALYCR",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self,num_classes):\n",
        "        super(Net, self).__init__()\n",
        "        self.encoder = encoder(res_block,[3,4,6,3],3)\n",
        "        self.in_channels=1024\n",
        "        self.bottle_conv1=nn.Conv2d(512,1024,kernel_size=3,stride=1,padding=1)\n",
        "        self.bottle_conv2=nn.Conv2d(1024,512,kernel_size=3,stride=1,padding=1)\n",
        "        self.relu=nn.ReLU()\n",
        "        self.unet_block1=unet_block(1024,256,256)\n",
        "        self.unet_block2=unet_block(512,128,128)\n",
        "        self.unet_block3=unet_block(256,64,64)\n",
        "        self.unet_block4=unet_block(128,64,64)\n",
        "        self.conv_transpose=nn.ConvTranspose2d(128,128,kernel_size=2,stride=2,padding=0)\n",
        "        self.out1=nn.Conv2d(128,num_classes,kernel_size=1,stride=1,padding=0)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x_skip,skip1,skip2,skip3,skip4=self.encoder(x)\n",
        "        x=self.bottle_conv1(x_skip)\n",
        "        x=self.relu(x)\n",
        "        x=self.bottle_conv2(x)\n",
        "        x=self.relu(x)\n",
        "        x=nn.ReLU()(torch.cat([x,x_skip],dim=1))\n",
        "        x=self.unet_block1(x,skip4)\n",
        "        x=self.unet_block2(x,skip3)\n",
        "        x=self.unet_block3(x,skip2)\n",
        "        x=self.unet_block4(x,skip1)\n",
        "        x=self.conv_transpose(x)\n",
        "        out1=self.out1(x)\n",
        "        return out1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXCEAnsvLYCW",
        "outputId": "42553421-7d28-435d-c963-2b072a48d467",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "encoder_decoder=Net(8)\n",
        "encoder_decoder.to(\"cuda:0\")\n",
        "summary(encoder_decoder,(3,224,224),device=\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaP1VHNL_OPN"
      },
      "source": [
        "## DATA-LOADER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "593-8Qe5LYCh",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class Utils:\n",
        "    @classmethod\n",
        "    def build_target(cls,target,num_classes):\n",
        "      target_map=np.transpose(to_categorical(target,num_classes=num_classes),axes=(0,3,1,2))\n",
        "      return target_map\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def get_loader(cls, image_size,batch_size,num_workers,CUDA):\n",
        "        \"\"\"Builds and returns Dataloader.\"\"\"\n",
        "        train_dataloader = DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True,num_workers=num_workers,pin_memory=False)\n",
        "        valid_dataloader = DataLoader(dataset=valid_dataset,batch_size=batch_size,shuffle=True,num_workers=num_workers,pin_memory=False)\n",
        "        return train_dataloader,valid_dataloader\n",
        "    \n",
        "    @classmethod\n",
        "    def print_metrics(cls,avg_loss, avg_acc):\n",
        "        t = PrettyTable(['Parameter', 'Value'])\n",
        "        t.add_row(['Avg_Loss', avg_loss])\n",
        "        t.add_row(['Avg_Acc', avg_acc])\n",
        "        print(t)\n",
        "        return\n",
        "    \n",
        "    @classmethod\n",
        "    def show_batch(cls,dl,batch):\n",
        "        j=0\n",
        "        for images, masks in dl:\n",
        "            j+=1\n",
        "            images=images.view(batch*15,224,224,3).contiguous()\n",
        "            masks=masks.view(batch*15,224,224).contiguous()\n",
        "            #os.mkdir(\"batch_\"+str(j))\n",
        "            for i in range(batch*15):\n",
        "                plt.imsave(os.path.join(\"batch_\"+str(j),\"batch_images\"+str(i)+\".png\"),np.array(images[i].detach().numpy(),dtype='uint8'))\n",
        "                plt.imsave(os.path.join(\"batch_\"+str(j),\"batch_masks\"+str(i)+\".png\"),masks[i].detach().numpy())\n",
        "        \n",
        "            break\n",
        "        return\n",
        "    \n",
        "    @classmethod\n",
        "    def shut_down_decoder(cls,model,flag):\n",
        "        i=0\n",
        "        for parameter in model.parameters():\n",
        "            parameter.requires_grad = flag\n",
        "            i+=1\n",
        "            if(i==108):\n",
        "                break\n",
        "        i=0\n",
        "        for parameter in model.parameters():\n",
        "            if parameter.requires_grad :\n",
        "                print(i,\"True\",parameter.data.shape)\n",
        "            else:\n",
        "                print(i,\"False\",parameter.shape)\n",
        "            i+=1\n",
        "        return \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scB4A4B75VcT",
        "outputId": "95a689c4-7fb4-45b8-97fd-ef836bdb25a1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "encoder_decoder=Net(num_classes=8)\n",
        "encoder_decoder.to(\"cuda:0\")\n",
        "#encoder_decoder.load_state_dict(torch.load(\"/content/drive/My Drive/encoder_decoder.pth\"),strict=False)\n",
        "Utils.shut_down_decoder(encoder_decoder,flag=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsvKF1aT_zf1"
      },
      "source": [
        "## LOSS AND METRICS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDqeP4p0LYCt",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#codes = np.loadtxt(path/'codes.txt', dtype=str)\n",
        "#name2id = {v:k for k,v in enumerate(codes)}\n",
        "void_code = 0\n",
        "\n",
        "class Metrics:\n",
        "\n",
        "    @classmethod\n",
        "    def acc_camvid(cls,input, target):\n",
        "      mask = target != void_code\n",
        "      return (torch.argmax(input, dim=1)[mask]==target[mask]).float().mean()\n",
        "\n",
        "    @classmethod\n",
        "    def compute_loss(cls,predictions, targets):\n",
        "        \"\"\"Calculating the loss and metrics\n",
        "        Args:\n",
        "            prediction = predicted image\n",
        "            target = Targeted image\n",
        "        Output:\n",
        "            loss : mse loss \"\"\"\n",
        "        ce_loss=torch.nn.CrossEntropyLoss()\n",
        "        c_loss=ce_loss(predictions,targets)\n",
        "        return c_loss\n",
        "    \n",
        "    @classmethod\n",
        "    def approx_acc(cls,prediction,target):\n",
        "        prediction = np.floor(prediction).astype(int).flatten()\n",
        "        target = np.floor(target).astype(int).flatten()\n",
        "        acc=accuracy_score(target,prediction)\n",
        "        return acc\n",
        "    \n",
        "    @classmethod\n",
        "    def max_error(cls,prediction,target):\n",
        "        prediction = prediction.flatten()\n",
        "        target = target.flatten()\n",
        "        maxerror=max_error(target,prediction)\n",
        "        return maxerror\n",
        "    \n",
        "    @classmethod\n",
        "    def var_score(cls,target,prediction):\n",
        "        var_score=explained_variance_score(target.flatten(),prediction.flatten())\n",
        "        return var_score\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgQFTcyH_7OZ"
      },
      "source": [
        "## TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6X1n2APK8ncy",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class TrainModel:\n",
        "    def __init__(self,batch_size=25,epochs=30,image_size=224,lr=0.001):\n",
        "        self.batch_size=batch_size\n",
        "        self.epochs=epochs\n",
        "        self.image_size=image_size\n",
        "        self.lr=lr\n",
        "        self.CUDA = torch.cuda.is_available()\n",
        "        self.device = \"cuda:0\"\n",
        "        \n",
        "    def fit(self,model):\n",
        "        model = model.to(self.device)\n",
        "        torch.save(model.state_dict(),\"/content/drive/MyDrive/DeepDrive/our_model_1.pth\")\n",
        "        self.optimizer=torch.optim.Adam(model.parameters(), lr=self.lr)\n",
        "        best_loss=0.7\n",
        "        (train_dataloader,valid_dataloader)=Utils.get_loader(image_size=self.image_size,batch_size=self.batch_size,num_workers=0,CUDA=self.CUDA)\n",
        "        print(\"Training\")\n",
        "        print(len(valid_dataloader))\n",
        "        for epoch in tqdm.tqdm(range(self.epochs)):\n",
        "            #Training Phase\n",
        "            model.train()\n",
        "            train_loss = []\n",
        "            train_acc = []\n",
        "            \n",
        "            for i, (images, masks) in enumerate(train_dataloader):\n",
        "                #images = images.view(images.shape[0],self.image_size,self.image_size,3)\n",
        "                #masks = masks.view(masks.shape[0],self.image_size,self.image_size)\n",
        "                images=images/255\n",
        "                images.to(self.device)\n",
        "                masks.to(self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = model(images)\n",
        "                loss = Metrics.compute_loss(outputs,masks)\n",
        "                print(loss)\n",
        "                train_loss.append(loss.item())\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                acc=Metrics.acc_camvid(outputs,masks)\n",
        "                train_acc.append(acc.item())\n",
        "            #Validation Phase\n",
        "            model.eval()\n",
        "            valid_loss = []\n",
        "            valid_acc = []\n",
        "            #print(\"Validation\")\n",
        "            for i, (images, masks) in enumerate(valid_dataloader):\n",
        "\n",
        "                \"\"\"images = images.view(images.shape[0],self.image_size,self.image_size,3)\n",
        "                masks = masks.view(masks.shape[0],self.image_size,self.image_size)\"\"\"\n",
        "                images=images/255\n",
        "\n",
        "                images.to(self.device)\n",
        "                masks.to(self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = encoder_decoder(images)\n",
        "                #print(\"outputs\",outputs.shape)\n",
        "               \n",
        "                loss = Metrics.compute_loss(outputs,masks)\n",
        "                #print(\"loss\",loss,end=\" \")\n",
        "                valid_loss.append(loss.item())\n",
        "                \n",
        "                acc=Metrics.acc_camvid(outputs,masks)\n",
        "                valid_acc.append(acc.item())\n",
        "            \n",
        "            avg_train_loss = np.average(train_loss)\n",
        "            avg_train_acc = np.average(train_acc)\n",
        "            Utils.print_metrics(avg_train_loss,avg_train_acc)\n",
        "            \n",
        "            avg_valid_loss = np.average(valid_loss)\n",
        "            avg_valid_acc = np.average(valid_acc)\n",
        "            Utils.print_metrics(avg_valid_loss,avg_valid_acc)\n",
        "            \n",
        "            \n",
        "            if(avg_valid_loss<best_loss):\n",
        "              torch.save(model.state_dict(),\"/content/drive/MyDrive/DeepDrive/our_model_\"+str(avg_valid_loss)+\".pth\")\n",
        "              best_loss = avg_valid_loss\n",
        "            \n",
        "        return\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcB63Vyh97bA",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "train=TrainModel(batch_size=10,epochs=5,image_size=224,lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPBOM3IK98ql",
        "outputId": "5558326b-75d3-44d7-acca-aa4d375f83a3",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "train.epochs=30\n",
        "train.image_size=224\n",
        "train.batch_size=12\n",
        "train.lr=0.003\n",
        "train.fit(encoder_decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JK2vkOwaI6-9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BERKLEY_DEEP_DRIVE.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1ce2269d4dc3421795527c77add8c2e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4c9f462a3fd4987a5dcd1d52f1d02f6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b1e4d8c4f9974117b9c838ab1df75a6d",
            "value": " 83.3M/83.3M [02:03&lt;00:00, 706kB/s]"
          }
        },
        "8224f5be8e0c45b4917a1e83471cc889": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "a2d5078ac55745f39c962cc58da5c9ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1e4d8c4f9974117b9c838ab1df75a6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4c9f462a3fd4987a5dcd1d52f1d02f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3e16f52c67f47f79c0d2778b5a1082f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e15747ea95e44b25b65495fb2b4291eb",
              "IPY_MODEL_1ce2269d4dc3421795527c77add8c2e8"
            ],
            "layout": "IPY_MODEL_a2d5078ac55745f39c962cc58da5c9ed"
          }
        },
        "e15747ea95e44b25b65495fb2b4291eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edecba93855141c4a73e62251a05d408",
            "max": 87306240,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8224f5be8e0c45b4917a1e83471cc889",
            "value": 87306240
          }
        },
        "edecba93855141c4a73e62251a05d408": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
